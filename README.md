# ğŸ¤– GitHub RAG Assistant

A local-first GitHub Repository Q&A system built using Retrieval-Augmented Generation (RAG).
It allows you to ask questions about any GitHub repository and get accurate answers grounded in the repositoryâ€™s source code.

ğŸ”’ Privacy-friendly â€” Code embeddings are generated locally using Ollama.
âš¡ Fast & Accurate â€” Uses ChromaDB vector search + Gemini 2.5 Flash for reasoning.

# ğŸš€ Features

ğŸ” Ask natural-language questions about any GitHub repository

ğŸ§  RAG-based architecture (no hallucinations)

ğŸ§© Intelligent code chunking & parsing

ğŸ§  Local embeddings via Ollama (nomic-embed-text)

ğŸ’¾ Persistent vector storage using ChromaDB

ğŸ’¬ High-quality answers using Google Gemini 2.5 Flash

ğŸ–¥ï¸ Interactive UI built with Streamlit

# âš™ï¸ Prerequisites
1ï¸âƒ£ Python
Python 3.9 â€“ 3.12

2ï¸âƒ£ Ollama (Required for Local Embeddings)

Install Ollama:
ğŸ‘‰ https://ollama.com/download

How It Works

User enters a GitHub repository URL

Repository is cloned locally

Source code is parsed & chunked

Embeddings are generated locally via Ollama

Vectors are stored in ChromaDB

User asks a question

Relevant code chunks are retrieved

Gemini 2.5 Flash generates a grounded answer

# ğŸ” Privacy & Security

âœ… No source code sent to embedding APIs

âœ… Embeddings generated locally

âœ… Vector DB stored on disk

âŒ No telemetry (disabled in ChromaDB)

# ğŸ› ï¸ Configuration

All settings are centralized in:

config/settings.py


You can configure:

Chunk size

Embedding batch size

Top-K retrieval

ChromaDB path

Model names

# ğŸ™Œ Acknowledgements

Ollama

ChromaDB

Sentence Transformers research

Google Gemini

